{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOvbgvJxHCgN5S5NtUKVFdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikhitaB15/Intrusion-Detection-System/blob/main/IDS_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jYErI2aP5c0",
        "outputId": "5f1694d0-7496-4447-93ae-6e19348b2866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from ids_model.pkl...\n",
            "Model loaded successfully!\n",
            "\n",
            "Starting real-time intrusion detection system...\n",
            "Available interfaces:\n",
            "0: lo\n",
            "1: eth0\n",
            "Stopping IDS...\n",
            "Stopping packet sniffer...\n",
            "IDS stopped.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import scapy.all as scapy\n",
        "import threading\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# ------ PART 1: DATA LOADING AND PREPROCESSING ------\n",
        "\n",
        "def load_nsl_kdd_data(train_path='/content/KDDTrain+.txt', test_path='/content/KDDTest+.txt'):\n",
        "    \"\"\"Load NSL-KDD dataset and perform initial preprocessing\"\"\"\n",
        "\n",
        "    # Column names based on NSL-KDD dataset documentation\n",
        "    columns = [\n",
        "        'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
        "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
        "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
        "        'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "        'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "        'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
        "        'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "        'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "        'dst_host_srv_rerror_rate', 'attack_type', 'difficulty_level'\n",
        "    ]\n",
        "\n",
        "    # Load the datasets\n",
        "    print(\"Loading NSL-KDD dataset...\")\n",
        "    try:\n",
        "        train_data = pd.read_csv(train_path, header=None, names=columns)\n",
        "        test_data = pd.read_csv(test_path, header=None, names=columns)\n",
        "\n",
        "        # Combine train and test for preprocessing\n",
        "        combined_data = pd.concat([train_data, test_data], axis=0)\n",
        "\n",
        "        # Categorize attacks into 5 main classes\n",
        "        attack_mapping = {\n",
        "            'normal': 'normal',\n",
        "            'back': 'dos', 'land': 'dos', 'neptune': 'dos', 'pod': 'dos',\n",
        "            'smurf': 'dos', 'teardrop': 'dos', 'mailbomb': 'dos', 'apache2': 'dos',\n",
        "            'processtable': 'dos', 'udpstorm': 'dos', 'worm': 'dos',\n",
        "            'ipsweep': 'probe', 'nmap': 'probe', 'portsweep': 'probe', 'satan': 'probe',\n",
        "            'mscan': 'probe', 'saint': 'probe',\n",
        "            'ftp_write': 'r2l', 'guess_passwd': 'r2l', 'imap': 'r2l', 'multihop': 'r2l',\n",
        "            'phf': 'r2l', 'spy': 'r2l', 'warezclient': 'r2l', 'warezmaster': 'r2l',\n",
        "            'sendmail': 'r2l', 'named': 'r2l', 'snmpgetattack': 'r2l', 'snmpguess': 'r2l',\n",
        "            'xlock': 'r2l', 'xsnoop': 'r2l', 'httptunnel': 'r2l',\n",
        "            'buffer_overflow': 'u2r', 'loadmodule': 'u2r', 'perl': 'u2r', 'rootkit': 'u2r',\n",
        "            'ps': 'u2r', 'sqlattack': 'u2r', 'xterm': 'u2r'\n",
        "        }\n",
        "\n",
        "        # Create attack category column\n",
        "        combined_data['attack_category'] = combined_data['attack_type'].map(\n",
        "            lambda x: attack_mapping.get(x.lower(), 'unknown')\n",
        "        )\n",
        "\n",
        "        # Binary classification: normal vs attack\n",
        "        combined_data['is_attack'] = (combined_data['attack_category'] != 'normal').astype(int)\n",
        "\n",
        "        return combined_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        print(\"Please make sure the NSL-KDD dataset is downloaded and paths are correct.\")\n",
        "        return None\n",
        "\n",
        "def engineer_features(data):\n",
        "    \"\"\"Perform feature engineering on the NSL-KDD dataset\"\"\"\n",
        "\n",
        "    print(\"Performing feature engineering...\")\n",
        "\n",
        "    # Create feature for ratio of source to destination bytes\n",
        "    data['bytes_ratio'] = data['src_bytes'] / (data['dst_bytes'] + 1)  # Adding 1 to avoid division by zero\n",
        "\n",
        "    # Connection rate features\n",
        "    data['error_rate_sum'] = data['serror_rate'] + data['rerror_rate']\n",
        "    data['srv_error_rate_sum'] = data['srv_serror_rate'] + data['srv_rerror_rate']\n",
        "    data['host_error_rate_sum'] = data['dst_host_serror_rate'] + data['dst_host_rerror_rate']\n",
        "\n",
        "    # Create a feature for connection time\n",
        "    data['is_long_connection'] = (data['duration'] > 300).astype(int)\n",
        "\n",
        "    # Feature for data transfer rate\n",
        "    data['transfer_rate'] = (data['src_bytes'] + data['dst_bytes']) / (data['duration'] + 1)\n",
        "\n",
        "    # Feature for login attempt success rate\n",
        "    data['login_success_rate'] = data['logged_in'] / (data['num_failed_logins'] + 1)\n",
        "\n",
        "    # Feature for same service rate consistency\n",
        "    data['service_consistency'] = abs(data['same_srv_rate'] - data['dst_host_same_srv_rate'])\n",
        "\n",
        "    # Feature to detect port scanning behavior\n",
        "    data['port_scan_indicator'] = ((data['count'] > 3) &\n",
        "                                 (data['srv_count'] == 1) &\n",
        "                                 (data['dst_host_srv_count'] < 5)).astype(int)\n",
        "\n",
        "    return data\n",
        "\n",
        "def prepare_data(data):\n",
        "    \"\"\"Prepare data for model training by splitting features and target\"\"\"\n",
        "\n",
        "    print(\"Preparing data for model training...\")\n",
        "\n",
        "    # Define features and target\n",
        "    X = data.drop(['attack_type', 'difficulty_level', 'attack_category', 'is_attack'], axis=1)\n",
        "    y = data['is_attack']  # Binary classification\n",
        "\n",
        "    # Define categorical and numerical features\n",
        "    categorical_features = ['protocol_type', 'service', 'flag']\n",
        "    numerical_features = [col for col in X.columns if col not in categorical_features]\n",
        "\n",
        "    # Create preprocessing pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "        ])\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, preprocessor\n",
        "\n",
        "# ------ PART 2: MODEL TRAINING ------\n",
        "\n",
        "def train_models(X_train, y_train, preprocessor):\n",
        "    \"\"\"Train Random Forest and Gradient Boosting models and create ensemble\"\"\"\n",
        "\n",
        "    print(\"Training models...\")\n",
        "\n",
        "    # Define models\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=20,\n",
        "        min_samples_split=10,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    gb_model = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create pipelines with preprocessing\n",
        "    rf_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', rf_model)\n",
        "    ])\n",
        "\n",
        "    gb_pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', gb_model)\n",
        "    ])\n",
        "\n",
        "    # Train individual models first\n",
        "    print(\"Training Random Forest model...\")\n",
        "    rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Training Gradient Boosting model...\")\n",
        "    gb_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Create ensemble model using voting\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf_pipeline),\n",
        "            ('gb', gb_pipeline)\n",
        "        ],\n",
        "        voting='soft'  # Use probability estimates for voting\n",
        "    )\n",
        "\n",
        "    # Train ensemble model\n",
        "    print(\"Training ensemble model...\")\n",
        "    ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "    return ensemble_model, rf_pipeline, gb_pipeline\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"Evaluate model performance\"\"\"\n",
        "\n",
        "    print(\"Evaluating model performance...\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Detailed classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "\n",
        "    return accuracy, y_pred\n",
        "\n",
        "# ------ PART 3: REAL-TIME PACKET SNIFFING WITH SCAPY ------\n",
        "\n",
        "class NetworkSniffer:\n",
        "    \"\"\"Class for real-time packet sniffing and analysis\"\"\"\n",
        "\n",
        "    def __init__(self, model, preprocessor):\n",
        "        self.model = model\n",
        "        self.preprocessor = preprocessor\n",
        "        self.is_running = False\n",
        "        self.packet_buffer = []\n",
        "        self.buffer_size = 100  # Number of packets to analyze at once\n",
        "        self.analysis_interval = 10  # Analysis interval in seconds\n",
        "\n",
        "    def start_sniffing(self, interface=None):\n",
        "        \"\"\"Start the packet sniffer\"\"\"\n",
        "\n",
        "        self.is_running = True\n",
        "        self.sniffer_thread = threading.Thread(target=self._sniff_packets, args=(interface,))\n",
        "        self.analyzer_thread = threading.Thread(target=self._analyze_packets)\n",
        "\n",
        "        print(f\"Starting packet sniffer on interface: {interface or 'default'}\")\n",
        "        self.sniffer_thread.daemon = True\n",
        "        self.analyzer_thread.daemon = True\n",
        "\n",
        "        self.sniffer_thread.start()\n",
        "        self.analyzer_thread.start()\n",
        "\n",
        "    def stop_sniffing(self):\n",
        "        \"\"\"Stop the packet sniffer\"\"\"\n",
        "\n",
        "        print(\"Stopping packet sniffer...\")\n",
        "        self.is_running = False\n",
        "        if hasattr(self, 'sniffer_thread'):\n",
        "            self.sniffer_thread.join(timeout=1)\n",
        "        if hasattr(self, 'analyzer_thread'):\n",
        "            self.analyzer_thread.join(timeout=1)\n",
        "\n",
        "    def _sniff_packets(self, interface):\n",
        "        \"\"\"Sniff packets using Scapy\"\"\"\n",
        "\n",
        "        # Use Scapy to capture packets\n",
        "        try:\n",
        "            scapy.sniff(\n",
        "                iface=interface,\n",
        "                prn=self._process_packet,\n",
        "                store=False,\n",
        "                stop_filter=lambda _: not self.is_running\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error sniffing packets: {e}\")\n",
        "            self.is_running = False\n",
        "\n",
        "    def _process_packet(self, packet):\n",
        "        \"\"\"Process captured packet and store features\"\"\"\n",
        "\n",
        "        # Extract features from packet\n",
        "        features = self._extract_packet_features(packet)\n",
        "\n",
        "        # Add to buffer\n",
        "        if features:\n",
        "            self.packet_buffer.append(features)\n",
        "\n",
        "            # If buffer is full, signal analysis\n",
        "            if len(self.packet_buffer) >= self.buffer_size:\n",
        "                self._analyze_current_buffer()\n",
        "\n",
        "    def _extract_packet_features(self, packet):\n",
        "        \"\"\"Extract relevant features from a packet for IDS analysis\"\"\"\n",
        "\n",
        "        features = {}\n",
        "\n",
        "        # Basic packet info\n",
        "        if scapy.IP in packet:\n",
        "            ip = packet[scapy.IP]\n",
        "            features['protocol_type'] = ip.proto\n",
        "            features['src_bytes'] = len(packet)\n",
        "            features['dst_bytes'] = 0  # Can't know response size from single packet\n",
        "\n",
        "            # TCP specific features\n",
        "            if scapy.TCP in packet:\n",
        "                tcp = packet[scapy.TCP]\n",
        "                features['service'] = tcp.dport\n",
        "                features['flag'] = tcp.flags\n",
        "                features['src_port'] = tcp.sport\n",
        "                features['dst_port'] = tcp.dport\n",
        "\n",
        "            # UDP specific features\n",
        "            elif scapy.UDP in packet:\n",
        "                udp = packet[scapy.UDP]\n",
        "                features['service'] = udp.dport\n",
        "                features['flag'] = 0\n",
        "                features['src_port'] = udp.sport\n",
        "                features['dst_port'] = udp.dport\n",
        "\n",
        "            # ICMP specific features\n",
        "            elif scapy.ICMP in packet:\n",
        "                features['service'] = 0\n",
        "                features['flag'] = 0\n",
        "                features['src_port'] = 0\n",
        "                features['dst_port'] = 0\n",
        "\n",
        "            # Default placeholder values for required NSL-KDD features\n",
        "            # These would be calculated based on connection tracking in a real IDS\n",
        "            features['duration'] = 0\n",
        "            features['land'] = 0\n",
        "            features['wrong_fragment'] = 0\n",
        "            features['urgent'] = 0\n",
        "            features['hot'] = 0\n",
        "            features['num_failed_logins'] = 0\n",
        "            features['logged_in'] = 0\n",
        "            features['num_compromised'] = 0\n",
        "            features['root_shell'] = 0\n",
        "            features['su_attempted'] = 0\n",
        "            features['num_root'] = 0\n",
        "            features['num_file_creations'] = 0\n",
        "            features['num_shells'] = 0\n",
        "            features['num_access_files'] = 0\n",
        "            features['num_outbound_cmds'] = 0\n",
        "            features['is_host_login'] = 0\n",
        "            features['is_guest_login'] = 0\n",
        "            features['count'] = 1\n",
        "            features['srv_count'] = 1\n",
        "\n",
        "            # Return the feature dictionary\n",
        "            return features\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _analyze_packets(self):\n",
        "        \"\"\"Continuously analyze buffered packets\"\"\"\n",
        "\n",
        "        while self.is_running:\n",
        "            if len(self.packet_buffer) > 0:\n",
        "                self._analyze_current_buffer()\n",
        "            time.sleep(self.analysis_interval)\n",
        "\n",
        "    def _analyze_current_buffer(self):\n",
        "        \"\"\"Analyze current packet buffer for intrusions\"\"\"\n",
        "\n",
        "        if not self.packet_buffer:\n",
        "            return\n",
        "\n",
        "        print(f\"Analyzing {len(self.packet_buffer)} packets...\")\n",
        "\n",
        "        try:\n",
        "            # Convert buffer to DataFrame\n",
        "            df = pd.DataFrame(self.packet_buffer)\n",
        "\n",
        "            # Fill missing values\n",
        "            df = df.fillna(0)\n",
        "\n",
        "            # Add engineered features from the model training\n",
        "            # In a real implementation, we would calculate these based on connection tracking\n",
        "            df['bytes_ratio'] = df['src_bytes'] / (df['dst_bytes'] + 1)\n",
        "            df['error_rate_sum'] = 0\n",
        "            df['srv_error_rate_sum'] = 0\n",
        "            df['host_error_rate_sum'] = 0\n",
        "            df['is_long_connection'] = 0\n",
        "            df['transfer_rate'] = df['src_bytes'] / 1\n",
        "            df['login_success_rate'] = 0\n",
        "            df['service_consistency'] = 0\n",
        "            df['port_scan_indicator'] = 0\n",
        "\n",
        "            # Add other required columns with default values\n",
        "            for col in ['serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "                       'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "                       'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                       'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "                       'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
        "                       'dst_host_srv_rerror_rate']:\n",
        "                if col not in df.columns:\n",
        "                    df[col] = 0\n",
        "\n",
        "            # Make predictions\n",
        "            predictions = self.model.predict(df)\n",
        "            probabilities = self.model.predict_proba(df)\n",
        "\n",
        "            # Count attacks detected\n",
        "            attack_count = np.sum(predictions == 1)\n",
        "\n",
        "            if attack_count > 0:\n",
        "                print(f\"⚠️ ALERT: Detected {attack_count} potential attacks out of {len(predictions)} packets!\")\n",
        "\n",
        "                # Get indices of potential attacks\n",
        "                attack_indices = np.where(predictions == 1)[0]\n",
        "\n",
        "                # Log attack details\n",
        "                for idx in attack_indices:\n",
        "                    attack_prob = probabilities[idx][1]\n",
        "                    packet_data = self.packet_buffer[idx]\n",
        "                    print(f\"Attack probability: {attack_prob:.4f}\")\n",
        "                    print(f\"Packet details: src_bytes={packet_data.get('src_bytes')}, \"\n",
        "                          f\"protocol={packet_data.get('protocol_type')}, \"\n",
        "                          f\"service={packet_data.get('service')}\")\n",
        "                    print(\"-\" * 50)\n",
        "            else:\n",
        "                print(\"No attacks detected in current buffer.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing packets: {e}\")\n",
        "\n",
        "        # Clear buffer\n",
        "        self.packet_buffer = []\n",
        "\n",
        "# ------ PART 4: MAIN APPLICATION ------\n",
        "\n",
        "def save_model(model, filename='ids_model.pkl'):\n",
        "    \"\"\"Save trained model to file\"\"\"\n",
        "\n",
        "    print(f\"Saving model to {filename}...\")\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "def load_model(filename='ids_model.pkl'):\n",
        "    \"\"\"Load trained model from file\"\"\"\n",
        "\n",
        "    print(f\"Loading model from {filename}...\")\n",
        "    try:\n",
        "        with open(filename, 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        return model\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model file {filename} not found.\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the IDS\"\"\"\n",
        "\n",
        "    # Check if model exists\n",
        "    model_file = 'ids_model.pkl'\n",
        "    if os.path.exists(model_file):\n",
        "        # Load existing model\n",
        "        model = load_model(model_file)\n",
        "        print(\"Model loaded successfully!\")\n",
        "    else:\n",
        "        # Train new model\n",
        "        print(\"No existing model found. Training new model...\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        data = load_nsl_kdd_data()\n",
        "        if data is None:\n",
        "            print(\"Failed to load dataset. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Engineer features\n",
        "        data = engineer_features(data)\n",
        "\n",
        "        # Prepare data for training\n",
        "        X_train, X_test, y_train, y_test, preprocessor = prepare_data(data)\n",
        "\n",
        "        # Train models\n",
        "        ensemble_model, rf_pipeline, gb_pipeline = train_models(X_train, y_train, preprocessor)\n",
        "\n",
        "        # Evaluate models (now they're properly fitted)\n",
        "        print(\"\\n--- Random Forest Model Evaluation ---\")\n",
        "        rf_accuracy, _ = evaluate_model(rf_pipeline, X_test, y_test)\n",
        "\n",
        "        print(\"\\n--- Gradient Boosting Model Evaluation ---\")\n",
        "        gb_accuracy, _ = evaluate_model(gb_pipeline, X_test, y_test)\n",
        "\n",
        "        print(\"\\n--- Ensemble Model Evaluation ---\")\n",
        "        ensemble_accuracy, _ = evaluate_model(ensemble_model, X_test, y_test)\n",
        "\n",
        "        # Save model\n",
        "        save_model(ensemble_model, model_file)\n",
        "        model = ensemble_model\n",
        "\n",
        "    # Start real-time packet sniffing\n",
        "    print(\"\\nStarting real-time intrusion detection system...\")\n",
        "    sniffer = NetworkSniffer(model, None)  # We pass None as preprocessor is part of the pipeline\n",
        "\n",
        "    try:\n",
        "        # Get interface from user\n",
        "        print(\"Available interfaces:\")\n",
        "        for i, iface in enumerate(scapy.get_if_list()):\n",
        "            print(f\"{i}: {iface}\")\n",
        "\n",
        "        interface_idx = input(\"Select interface number (or press Enter for default): \")\n",
        "        interface = None\n",
        "        if interface_idx.strip():\n",
        "            interface = scapy.get_if_list()[int(interface_idx)]\n",
        "\n",
        "        # Start sniffing\n",
        "        sniffer.start_sniffing(interface)\n",
        "\n",
        "        # Run until user stops\n",
        "        print(\"IDS is running. Press Ctrl+C to stop.\")\n",
        "        while True:\n",
        "            time.sleep(1)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Stopping IDS...\")\n",
        "    finally:\n",
        "        sniffer.stop_sniffing()\n",
        "        print(\"IDS stopped.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqgrtNTAQioV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}